# Visual Question Answering (VQA) Multimodal AI System

A state-of-the-art Visual Question Answering system that combines computer vision and natural language processing to answer questions about images. Built with **Lightning AI platform** for scalable training and deployment, this project integrates CLIP for image understanding and Mistral-7B for language processing, utilizing LoRA for efficient fine-tuning.

## ğŸš€ Features

- **Lightning AI Orchestration**: Complete ML pipeline management with `LightningApp` and `LightningFlow`
- **Multimodal Architecture**: CLIP-ViT-Base-Patch32 + Mistral-7B-v0.1 integration
- **Efficient Fine-tuning**: LoRA (Low-Rank Adaptation) for parameter-efficient training
- **Production Ready**: FastAPI backend with RESTful API endpoints
- **Interactive Interface**: Gradio web demo for real-time user interaction
- **Memory Optimized**: BitsAndBytes quantization for reduced VRAM usage
- **Structured Training**: PyTorch Lightning modules for robust training loops

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Input Image   â”‚    â”‚   Text Question â”‚    â”‚   Lightning AI  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                      â”‚                      â”‚
          â–¼                      â–¼                      â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  CLIP   â”‚              â”‚Mistral-7Bâ”‚          â”‚Workflow â”‚
    â”‚Encoder  â”‚              â”‚   LLM   â”‚          â”‚Manager  â”‚
    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                        â”‚
          â–¼                        â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚    Multimodal Fusion Layer      â”‚
    â”‚         (LoRA Adapted)          â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚ Answer  â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“‹ Requirements

- Python 3.8+
- CUDA-capable GPU (recommended: 16GB+ VRAM)
- Lightning AI account (for platform features)

## ğŸ› ï¸ Installation

1. **Clone the Repository**
   ```
   git clone https://github.com/madhav-051429/vqa_project.git
   cd vqa_project
   ```

2. **Install Dependencies**
   ```
   pip install -r requirements.txt
   ```

3. **Set Up Lightning AI** (Optional for local use)
   ```
   lightning login
   ```

## ğŸ“ Project Structure

```
vqa_project/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api.py                 # FastAPI backend
â”‚   â””â”€â”€ gradio_interface.py    # Gradio web interface
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train_clip.py         # CLIP training script
â”‚   â””â”€â”€ train_llama.py        # LLaMA training script
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/               # Model definitions
â”‚   â”œâ”€â”€ data/                 # Data loading utilities
â”‚   â””â”€â”€ utils/                # Helper functions
â”œâ”€â”€ lightning_app.py          # Lightning AI orchestrator
â”œâ”€â”€ main.py                   # Main application entry
â””â”€â”€ requirements.txt          # Dependencies
```

## ğŸš€ Usage

### Local Development

1. **Run the FastAPI Backend**
   ```
   uvicorn app.api:app --reload --host 0.0.0.0 --port 8000
   ```

2. **Launch Gradio Interface**
   ```
   python app/gradio_interface.py
   ```

3. **Access the Application**
   - API Documentation: `http://localhost:8000/docs`
   - Gradio Demo: `http://localhost:7860`

### Lightning AI Platform

1. **Run the Lightning App**
   ```
   lightning run app lightning_app.py
   ```

2. **Monitor Training**
   ```
   lightning run app lightning_app.py --cloud
   ```

### Training

1. **Train CLIP Model**
   ```
   python scripts/train_clip.py --epochs 10 --batch_size 32
   ```

2. **Fine-tune Mistral with LoRA**
   ```
   python scripts/train_llama.py --lora_rank 16 --learning_rate 5e-5
   ```

## ğŸ’¡ API Usage

### Predict Endpoint

```
curl -X POST "http://localhost:8000/predict" \
     -H "Content-Type: application/json" \
     -d '{
       "image_url": "https://example.com/image.jpg",
       "question": "What is in this image?"
     }'
```

### Python Example

```python
import requests

response = requests.post(
    "http://localhost:8000/predict",
    json={
        "image_url": "path/to/image.jpg",
        "question": "How many people are in the image?"
    }
)
print(response.json())
```

## ğŸ“Š Model Performance

- **Dataset**: VQA v2.0
- **Architecture**: CLIP + Mistral-7B with LoRA
- **Training**: Parameter-efficient fine-tuning
- **Inference**: Optimized for real-time performance

## ğŸ”§ Configuration

Key configuration parameters in `config.py`:

```python
MODEL_CONFIG = {
    "clip_model": "openai/clip-vit-base-patch32",
    "llm_model": "mistralai/Mistral-7B-v0.1",
    "lora_rank": 16,
    "quantization": "4bit",
    "max_length": 512
}
```

## ğŸ› Troubleshooting

### Common Issues

1. **CUDA Out of Memory**
   - Reduce batch size
   - Enable gradient checkpointing
   - Use smaller LoRA rank

2. **Model Loading Errors**
   - Check internet connection for model downloads
   - Verify transformer library version compatibility
   - Ensure sufficient disk space

3. **Lightning AI Connection Issues**
   - Verify Lightning AI credentials
   - Check network connectivity
   - Update Lightning CLI

### Memory Requirements

- **Training**: 16GB+ VRAM recommended
- **Inference**: 8GB+ VRAM minimum
- **CPU Inference**: 32GB+ RAM (slower performance)

## ğŸ§ª Testing

Run the test suite:
```
python -m pytest tests/ -v
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“š References

- [CLIP Paper](https://arxiv.org/abs/2103.00020)
- [LoRA Paper](https://arxiv.org/abs/2106.09685)
- [VQA v2 Dataset](https://visualqa.org/)
- [Lightning AI Documentation](https://lightning.ai/docs/)


---

â­ **Star this repository if you found it helpful!**
